m = 1,
maxit = 5,
method = "cart",
threshold = 2) #This threshold means we keep in all the highly correlated variables
#This makes a data-set with the respective NA values computed
data.imp.data = complete(data.imp,1)
#rename the data-set so that we can use it going forward in the code
data.recode = data.imp.data
} else{
#list-wise deletion of all dummy NA's
data.recode = na.omit(data.rm)
}
#now we need to do a check for the observation to variable ratio since we may have
#low observations after list-wise deletion
if (nrow(data.recode)/ncol(data.recode) < 5){
stop(paste("Observation to Variable ratio of (",round(nrow(data.recode)/ncol(data.recode),2),") is less than 5 after List-Wise Deletion (or has always been if using Multiple Imputation). Consider the following:",
"\n-Increase the function variable: 'naPercent'\n-Run missing value imputation by setting impFlag = TRUE\n-Use less variables\n-Gather more observations",sep=""))
}
#now we need to encode our continuous and categorical variables
#first we need to encode categorical as factors and normalize the continuous variables
#This loop will change variables with less than 'catLevels' levels into a factor otherwise normalize
#We also don't factorize the predictor variable since this will cause issues when making dummies
#we also don't scale the predictor variable IF it's categorical (would normally be a factor)
for (i in 1:length(data.recode)) {
#This check is needed if the DV is a factor or the variable has more levels than catLevels, turns into numerical
if (names(data.recode)[i] == predictVar | length(unique(data.recode[,i])) > catLevels) {
data.recode[,i] = as.numeric(data.recode[,i])
}
#This recodes a column into a factor
if (length(unique(data.recode[,i])) <= catLevels & names(data.recode)[i] != predictVar) {
data.recode[,i] = as.factor(data.recode[,i])
}
}
#loop goes though each columns, makes a freq table of the levels
print("----------Low Level Removal----------")
print("If a level is removed from a variable you wish to keep, reccomended to manually merge levels toegther and re-run")
#needed for console outputs
varCounter = 0
levelCounter = 0
obsCounter = 0
#start of loop
for (i in names(data.recode)) {
#checks if variable is a factor that is does have at least 1 level with less than obsPerLevel observations
if (is.factor(data.recode[[i]]) & !is_empty(names(which(table(data.recode[[i]])<obsPerLevel)))) {
varCounter = varCounter + 1
#looping though the freq table to find levels with less than obsPerLevel observations
for (j in names(which(table(data.recode[[i]])<obsPerLevel)) ) {
#console output
print(paste("level",j,"in",i,"removed,",table(data.recode[[i]])[j],"observations found"))
#before removing, we need to update the total obs removed count
obsCounter = obsCounter + ifelse(!is.na(table(data.recode[[i]])[j]),table(data.recode[[i]])[j],0)
#removing the level, list-wise deletion
data.recode = data.recode[data.recode[[i]] != j,]
#counting the number of levels
levelCounter = levelCounter + 1
}
}
}
library(rlang)
#start of loop
for (i in names(data.recode)) {
#checks if variable is a factor that is does have at least 1 level with less than obsPerLevel observations
if (is.factor(data.recode[[i]]) & !is_empty(names(which(table(data.recode[[i]])<obsPerLevel)))) {
varCounter = varCounter + 1
#looping though the freq table to find levels with less than obsPerLevel observations
for (j in names(which(table(data.recode[[i]])<obsPerLevel)) ) {
#console output
print(paste("level",j,"in",i,"removed,",table(data.recode[[i]])[j],"observations found"))
#before removing, we need to update the total obs removed count
obsCounter = obsCounter + ifelse(!is.na(table(data.recode[[i]])[j]),table(data.recode[[i]])[j],0)
#removing the level, list-wise deletion
data.recode = data.recode[data.recode[[i]] != j,]
#counting the number of levels
levelCounter = levelCounter + 1
}
}
}
#overall results from procedure
print(paste(levelCounter,"total levels removed from",varCounter,"different variables. In total",obsCounter,"observations deleted"))
#Check here since we may or removed enough observations to deem the ratio poor
if (nrow(data.recode)/ncol(data.recode) < 5){
stop(paste("Observation to Variable ratio of ",round(nrow(data.recode)/ncol(data.recode),2)," is less than 5 after Low Level Removal. Consider the following",
"\n-Increase the  function variable: 'naPercent'\n-Lower the value of function variable: 'obsPerLevel'\n-Use less variables\n-Gather more observations",sep=""))
}
#use the function to clean 0 var variables
data.recode.2 = var0Remove(data.recode)
#Here we need to check if the dependent variable is still apart of the data (if it's not it's because the DV had no variance)
if (sum(names(data.recode.2) == predictVar) == 0){
paste("Dependant Variable no longer has variance (all values the same). Consider the following:",
"\n-Lower the value of function variable: 'obsPerLevel'\n-Gather more observations for the DV\n-Attempt to group similar DV values to produce more observations per level",sep="")
}
#There are a few more steps before modelling, however we shouldn't apply these next steps to the DV, therefore we split it out
#set the Dependent Variable
y = select(data.recode.2,all_of(predictVar))
View(y)
#all the rest/all the predictor variables
data.recode.2 = select(data.recode.2,-all_of(predictVar))
View(data.recode.2)
View(iris)
#next we need to encode the categorical/factor variables as multiple columns of 0's and 1's to work with a matrix form
#the above step makes k-1 columns if a categorical variable has k levels (thanks to fullRank = T)
print("----------Dummy Variables----------")
dmy = dummyVars(" ~ .", data = data.recode.2, fullRank = T)
data.recode.dmy = data.frame(predict(dmy, newdata = data.recode.2))
library(caret)
#next we need to encode the categorical/factor variables as multiple columns of 0's and 1's to work with a matrix form
#the above step makes k-1 columns if a categorical variable has k levels (thanks to fullRank = T)
print("----------Dummy Variables----------")
dmy = dummyVars(" ~ .", data = data.recode.2, fullRank = T)
data.recode.dmy = data.frame(predict(dmy, newdata = data.recode.2))
print(paste("predictor variable count went from",ncol(data.recode.2),"to",ncol(data.recode.dmy)))
#After encoding the dummies, we need to remove 0 var variables again since some of the levels described by the factors in the data
#may no longer exist (which is what the dummy function uses to dumm-ize the data) therefore we end up with dummy columns of no variance
data.clus = var0Remove(data.recode.dmy)
clusterAmount
View(data.clus)
View(iris)
as.matrix(data.clus)
randomSeed = 3
###K-Means###
#we next do K-Means since removing correlated variables and scaling negatively effect this model
#we use all the data (both train and test) excluding the dependant variable
print("----------K-Means----------")
set.seed(randomSeed) #need to set random seed here to ensure same results if set
kmClus = kmeans(as.matrix(data.clus),clusterAmount,1000)
#Tell user how many clusters have been made
print(paste(clusterAmount,"clusters have been made for K-Means"))
print(table(kmClus$cluster,as.matrix(y)))
set.seed(randomSeed) #need to set random seed here to ensure same results if set
kmClus = kmeans(as.matrix(data.clus),clusterAmount,1000)
print(table(kmClus$cluster,as.matrix(y)))
set.seed(randomSeed) #need to set random seed here to ensure same results if set
kmClus = kmeans(as.matrix(data.clus),clusterAmount,100000)
#print table
print("K-Means results as a table, the max value in each row is a simple way to define which cluster represents which variable (ties can be decided by the user)")
print(table(kmClus$cluster,as.matrix(y)))
View(data.clus)
clusterAmount
kmClus = kmeans(data.clus,clusterAmount,1000)
print(table(kmClus$cluster,as.matrix(y)))
kmClus = kmeans(data.clus,clusterAmount,1000)
print(table(kmClus$cluster,y))
set.seed(randomSeed) #need to set random seed here to ensure same results if set
kmClus = kmeans(data.clus,clusterAmount,1000)
print(table(kmClus$cluster,y))
kmClus$cluster
temp = kmClus$cluster
print(table(kmClus$cluster,as.vector(y)))
print(table(kmClus$cluster,as.matrix(y)))
kmClus = kmeans(data.clus,clusterAmount,1000)
print(table(kmClus$cluster,as.matrix(y)))
print(table(kmClus$cluster,as.matrix(y)))
print(table(kmClus$cluster,y))
print(table(kmClus$cluster,as.matrix(y)))
detach("package:automodel", unload = TRUE)
install_local("automodel.zip")
library(automodel) #the function made during this dissertation, all code is within 'automodel.R'
#run the model with our context on correlation
iris.r = autoModel("Species", iris, randomSeed = 3, corrConfLevel = 0.99)
detach("package:automodel", unload = TRUE)
install_local("automodel.zip")
library(automodel) #the function made during this dissertation, all code is within 'automodel.R'
#run the model with our context on correlation
iris.r = autoModel("Species", iris, randomSeed = 3, corrConfLevel = 0.99)
#run the model
titanicData.r = autoModel("Survived", titanicData, randomSeed = 3)
########Titanic Run########
#Load in the Titanic data (notice we load in 'train.csv', that because the test data Kaggle provides doesn't have 'survived' values)
titanicData = read.csv("rData/TitanicData/train.csv")
#run the model (random seed 3 works as-well)
titanicData.r = autoModel("Survived", titanicData, randomSeed = 3)
mixNurse.r = autoModel("total_score", mixNurse.rData, randomSeed = 3)
kmNorm = kmeans(iris[1:4],3)
table(iris$Species,knNorm$cluster)
table(iris$Species,kmNorm$cluster)
kmMat = kmeans(as.matrix(iris[1:4]),3)
table(iris$Species,kmMat$cluster)
set.seed(1)
kmNorm = kmeans(iris[1:4],3)
table(iris$Species,kmNorm$cluster)
set.seed(1)
kmMat = kmeans(as.matrix(iris[1:4]),3)
table(iris$Species,kmMat$cluster)
kmNorm = kmeans(iris[1:4],3)
table(iris$Species,kmNorm$cluster)
kmNorm = kmeans(iris[1:4],3)
table(iris$Species,kmNorm$cluster)
kmNorm = kmeans(iris[1:4],3)
table(iris$Species,kmNorm$cluster)
kmMat = kmeans(as.matrix(iris[1:4]),3)
table(iris$Species,kmMat$cluster)
kmMat = kmeans(as.matrix(iris[1:4]),3)
table(iris$Species,kmMat$cluster)
kmMat = kmeans(as.matrix(iris[1:4]),3)
table(iris$Species,kmMat$cluster)
kmMat = kmeans(as.matrix(iris[1:4]),3)
table(iris$Species,kmMat$cluster)
kmNorm = kmeans(iris[1:4],3)
table(iris$Species,kmNorm$cluster)
irisNew = iris[1:4]
species = iris$Species
kmNorm = kmeans(iris[1:4],3)
table(kmNorm$cluster,iris$Species)
irisNew = iris[1:4]
species = iris$Species
kmMat = kmeans(as.matrix(irisNew),3)
table(kmMat$cluster,species)
table(kmMat$cluster,as.matrix(species))
#run the model with our context on correlation
iris.r = autoModel("Species", iris, randomSeed = 3, corrConfLevel = 0.99)
kmClus = kmeans(data.clus,clusterAmount,1000)
#Tell user how many clusters have been made
print(paste(clusterAmount,"clusters have been made for K-Means"))
print(table(kmClus$cluster,as.matrix(y)))
set.seed(randomSeed) #need to set random seed here to ensure same results if set
kmClus = kmeans(data.clus,clusterAmount,1000)
#Tell user how many clusters have been made
print(paste(clusterAmount,"clusters have been made for K-Means"))
#print table
print("K-Means results as a table, the max value in each row is a simple way to define which cluster represents which variable (ties can be decided by the user)")
print(table(kmClus$cluster,as.matrix(y)))
set.seed(randomSeed) #need to set random seed here to ensure same results if set
kmClus = kmeans(data.clus,clusterAmount,1000)
#print table
print("K-Means results as a table, the max value in each row is a simple way to define which cluster represents which variable (ties can be decided by the user)")
print(table(kmClus$cluster,as.matrix(y)))
randomSeed = 4
set.seed(randomSeed) #need to set random seed here to ensure same results if set
kmClus = kmeans(data.clus,clusterAmount,1000)
print(table(kmClus$cluster,as.matrix(y)))
randomSeed = 3
set.seed(randomSeed) #need to set random seed here to ensure same results if set
kmClus = kmeans(data.clus,clusterAmount,1000)
print(table(kmClus$cluster,as.matrix(y)))
#run the model with our context on correlation
iris.r = autoModel("Species", iris, randomSeed = 1, corrConfLevel = 0.99)
#set console output location
sink("consoleOutput/iris_1.txt", type=c("output","message"))
#run the model with our context on correlation
iris.r = autoModel("Species", iris, randomSeed = 1, corrConfLevel = 0.99)
#close the connection
closeAllConnections()
?iris
iris$Species
data(iris)
summary(iris)
data(iris)
#set console output location
sink("consoleOutput/iris.txt", type=c("output","message"))
#run the model with our context on correlation
iris.r = autoModel("Species", iris, randomSeed = 1, corrConfLevel = 0.99)
#close the connection
closeAllConnections()
#run the model with our context on correlation
iris.r = autoModel("Species", iris, randomSeed = 1, corrConfLevel = 0.99)
detach("package:automodel", unload = TRUE)
install_local("automodel.zip")
library(automodel) #the function made during this dissertation, all code is within 'automodel.R'
#run the model with our context on correlation
iris.r = autoModel("Species", iris, randomSeed = 1, corrConfLevel = 0.99)
detach("package:automodel", unload = TRUE)
install_local("automodel.zip")
library(automodel) #the function made during this dissertation, all code is within 'automodel.R'
#set console output location (needed for write up, you can un-comment to store console output in a text file)
sink("consoleOutput/w2indresp.txt", type=c("output","message"))
#generate models
w2indresp.r = autoModel("total_score", w2indresp.rData, randomSeed = 3)
#set console output location
sink("consoleOutput/w2Merge.txt", type=c("output","message"))
w2Merge.r = autoModel("total_score", w2Merge.rData, randomSeed = 3)
#running again due to error message of VIF calculation
sink("consoleOutput/w2MergeNurse_3.txt", type=c("output","message"))
w2MergeNurse.r = autoModel("total_score", w2MergeNurse.rData, randomSeed = 3, naPercent = 0.1, corrConfLevel = 0.5)
#re-run since original run threw Observation to variable Ratio error
sink("consoleOutput/w2MergeNurseBlood_2.txt", type=c("output","message"))
w2MergeNurseBlood.r = autoModel("total_score", w2MergeNurseBlood.rData, randomSeed = 3, naPercent = 0.05)
#second run
sink("consoleOutput/w3indresp_2.txt", type=c("output","message"))
w3indresp.r = autoModel("total_score", w3indresp.rData, randomSeed = 3, corrConfLevel = 0.5)
sink("consoleOutput/w3Merge_2.txt", type=c("output","message"))
w3Merge.r = autoModel("total_score", w3Merge.rData, randomSeed = 3, corrConfLevel = 0.5)
#re run since 3rd run threw VIF error
sink("consoleOutput/w3MergeNurse_4.txt", type=c("output","message"))
w3MergeNurse.r = autoModel("total_score", w3MergeNurse.rData, randomSeed = 3, naPercent = 0.009, obsPerLevel = 4, corrConfLevel = 0.5)
#given 3rd threw observation to variable ratio error
sink("consoleOutput/w3MergeNurseBlood_3.txt", type=c("output","message"))
#NOTE: obsPerLevel = 2 wouldn't run, threw observation to ratio error
w3MergeNurseBlood.r = autoModel("total_score", w3MergeNurseBlood.rData, randomSeed = 3, naPercent = 0.0001, obsPerLevel = 1)
#set console output location
sink("consoleOutput/wShared.txt", type=c("output","message"))
wShared.r = autoModel("total_score", wShared.rData, randomSeed = 3)
#set console output location
sink("consoleOutput/wSMerge_2.txt", type=c("output","message"))
wSMerge.r = autoModel("total_score", wSMerge.rData, randomSeed = 3, corrConfLevel = 0.5)
#set console output location
sink("consoleOutput/wSMergeNurse.txt", type=c("output","message"))
wSMergeNurse.r = autoModel("total_score", wSMergeNurse.rData, randomSeed = 3)
#given 1st run the observation to variable ratio error
sink("consoleOutput/wSMergeNurseBlood_2.txt", type=c("output","message"))
wSMergeNurseBlood.r = autoModel("total_score", wSMergeNurseBlood.rData, randomSeed = 3, naPercent = 0.15)
#set console output location
sink("consoleOutput/mixNurse.txt", type=c("output","message"))
mixNurse.r = autoModel("total_score", mixNurse.rData, randomSeed = 3)
#set console output location
sink("consoleOutput/mixNurseBlood.txt", type=c("output","message"))
mixNurseBlood.r = autoModel("total_score", mixNurseBlood.rData, randomSeed = 3)
#set console output location
sink("consoleOutput/w2MergeNurse_1.txt", type=c("output","message"))
w2MergeNurse.r = autoModel("total_score", w2MergeNurse.rData, randomSeed = 3)
closeAllConnections()
#running again due to error message of observation to variable ratio
sink("consoleOutput/w2MergeNurse_2.txt", type=c("output","message"))
w2MergeNurse.r = autoModel("total_score", w2MergeNurse.rData, randomSeed = 3, naPercent = 0.1)
#set console output location
sink("consoleOutput/w2MergeNurseBlood_1.txt", type=c("output","message"))
w2MergeNurseBlood.r = autoModel("total_score", w2MergeNurseBlood.rData, randomSeed = 3)
#set console output location
sink("consoleOutput/w3indresp_1.txt", type=c("output","message"))
#first attempt, throws an error saying the VIF can't be calculated, we adjust the corrConfLevel accordingly
w3indresp.r = autoModel("total_score", w3indresp.rData, randomSeed = 3)
#set console output location
sink("consoleOutput/w3Merge_1.txt", type=c("output","message"))
w3Merge.r = autoModel("total_score", w3Merge.rData, randomSeed = 3)
#set console output location
sink("consoleOutput/w3MergeNurse_1.txt", type=c("output","message"))
w3MergeNurse.r = autoModel("total_score", w3MergeNurse.rData, randomSeed = 3)
#re run since 1st run threw observation to variable error
sink("consoleOutput/w3MergeNurse_2.txt", type=c("output","message"))
w3MergeNurse.r = autoModel("total_score", w3MergeNurse.rData, randomSeed = 3, naPercent = 0.009)
#re run since 2nd run threw observation to variable error
sink("consoleOutput/w3MergeNurse_3.txt", type=c("output","message"))
w3MergeNurse.r = autoModel("total_score", w3MergeNurse.rData, randomSeed = 3, naPercent = 0.009, obsPerLevel = 4)
#set console output location
sink("consoleOutput/w3MergeNurseBlood_1.txt", type=c("output","message"))
w3MergeNurseBlood.r = autoModel("total_score", w3MergeNurseBlood.rData, randomSeed = 3)
#given 2nd threw observation to variable ratio error
sink("consoleOutput/w3MergeNurseBlood_2.txt", type=c("output","message"))
w3MergeNurseBlood.r = autoModel("total_score", w3MergeNurseBlood.rData, randomSeed = 3, naPercent = 0.0001)
#set console output location
sink("consoleOutput/wSMerge_1.txt", type=c("output","message"))
wSMerge.r = autoModel("total_score", wSMerge.rData, randomSeed = 3)
#set console output location
sink("consoleOutput/wSMergeNurseBlood_1.txt", type=c("output","message"))
wSMergeNurseBlood.r = autoModel("total_score", wSMergeNurseBlood.rData, randomSeed = 3)
closeAllConnections()
########Titanic Run########
#Load in the Titanic data (notice we load in 'train.csv', that because the test data Kaggle provides doesn't have 'survived' values)
titanicData = read.csv("rData/TitanicData/train.csv")
#get the random chances of picking Survival we are wanting to beat with our models
#complete random
randomCh.t = 1/length(unique(titanicData$Survived))
randomCh.t
#mode selection
mstComm.t = as.numeric(names(which(table(titanicData$Survived) == max(table(titanicData$Survived)))))
paste("Mode/most frequent score is:",mstComm.t)
modePickCh.t = max(table(titanicData$Survived)) / length(titanicData$Survived)
modePickCh.t
#we can also make the predictor variable a factor, this generates a legend we can follow in the model outputs
titanicData$Survived = as.factor(ifelse(titanicData$Survived == 0,"Didn't Survive","Survived"))
sink("consoleOutput/Titanic.txt", type=c("output","message"))
#run the model (random seed 3 works as-well)
titanicData.r = autoModel("Survived", titanicData, randomSeed = 3)
#This needs to be ran to close all the sink connections
closeAllConnections()
########Iris Run########
#Iris is a in-built data-set for R which we can test our function on
summary(iris)
#set console output location
sink("consoleOutput/iris.txt", type=c("output","message"))
#run the model with our context on correlation
iris.r = autoModel("Species", iris, randomSeed = 1, corrConfLevel = 0.99)
#close the connection
closeAllConnections()
####################################
#the shared wave participants
####################################
#analyzing this merge
ghq_analyze(wShared)
detach("package:automodel", unload = TRUE)
install_local("automodel.zip")
library(automodel) #the function made during this dissertation, all code is within 'automodel.R'
########House Data########
#The life expectnacy data is a dataset used within the module MA317 at UoE
house_sales = read.csv("rData/MA321_Data/house-data.csv")
#run the model for two different variables
house_sales.r.oc = autoModel("OverallCond", house_sales)
#given our package made isn't on CRAN and we have it locally, we need to install from the zip file using devtools
library(devtools)
install_local("automodel.zip")
install_local("automodel.zip")
install_local("automodel.zip")
library(automodel) #the function made during this dissertation, all code is within 'automodel.R'
########Titanic Run########
titanicData = read.csv("rData/TitanicData/train.csv")
########Titanic Run########
titanicData = read.csv("rData/TitanicData/train.csv")
titanicData.r = autoModel("Survived", titanicData)
########Titanic Run########
#https://www.kaggle.com/c/titanic
titanicData = read.csv("rData/TitanicData/train.csv")
titanicData.r = autoModel("Survived", titanicData)
########House Data########
#https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques
house_sales = read.csv("rData/MA321_Data/house-data.csv")
#run the model for two different variables
house_sales.r.oc = autoModel("OverallCond", house_sales)
house_sales.r.sp = autoModel("SalePrice", house_sales)
#showing the salePrice data
boxplot(house_sales$SalePrice)
#showing the salePrice data (outliers)
boxplot(house_sales$SalePrice,
main = "House Sale Price BoxPlot",
ylab = "SalePrice")
#showing the salePrice data (outliers)
boxplot(house_sales$SalePrice,
main = "House Sale Price BoxPlot",
ylab = "SalePrice")
rect(xleft = 7.5, xright = 8.5, ybottom = -2, ytop = 2, border = "red", lwd = 3)
rect(xleft = 400000, xright = 8.5, ybottom = -2, ytop = 2, border = "red", lwd = 3)
rect(xleft = 400000, xright = 8.5, ybottom = 400000, ytop = 500000, border = "red", lwd = 3)
rect(xleft = 0, xright = 1, ybottom = 400000, ytop = 500000, border = "red", lwd = 3)
rect(xleft = 0.5, xright = 1.5, ybottom = 400000, ytop = 500000, border = "red", lwd = 3)
rect(xleft = 0.5, xright = 1.5, ybottom = 320000, ytop = 750000, border = "red", lwd = 3)
rect(xleft = 0.9, xright = 1.1, ybottom = 320000, ytop = 750000, border = "red", lwd = 3)
rect(xleft = 0.9, xright = 1.1, ybottom = 320000, ytop = 800000, border = "red", lwd = 3)
rect(xleft = 0.9, xright = 1.1, ybottom = 320000, ytop = 780000, border = "red", lwd = 3)
rect(xleft = 0.9, xright = 1.1, ybottom = 320000, ytop = 770000, border = "red", lwd = 3)
#showing the salePrice data (outliers)
boxplot(house_sales$SalePrice,
main = "House Sale Price BoxPlot",
ylab = "SalePrice")
rect(xleft = 0.9, xright = 1.1, ybottom = 320000, ytop = 770000, border = "red", lwd = 3)
#showing the salePrice data (outliers)
boxplot(house_sales$SalePrice,
main = "House Sale Price BoxPlot",
ylab = "SalePrice")
rect(xleft = 0.8, xright = 1.2, ybottom = 320000, ytop = 770000, border = "red", lwd = 3)
#showing the salePrice data (outliers)
boxplot(house_sales$SalePrice,
main = "House Sale Price BoxPlot",
ylab = "SalePrice")
rect(xleft = 0.85, xright = 1.15, ybottom = 320000, ytop = 770000, border = "red", lwd = 3)
#showing the salePrice data (outliers)
boxplot(house_sales$SalePrice,
main = "House Sale Price Boxplot (Outliers Outlied)",
ylab = "SalePrice")
rect(xleft = 0.85, xright = 1.15, ybottom = 320000, ytop = 770000, border = "red", lwd = 3)
#showing the salePrice data (outliers)
boxplot(house_sales$SalePrice,
main = "House Sale Price Boxplot",
ylab = "SalePrice")
rect(xleft = 0.85, xright = 1.15, ybottom = 320000, ytop = 770000, border = "red", lwd = 3)
house_sales = read.csv("rData/MA321_Data/house-data.csv")
View(house_sales)
boxplot(SalesPrice ~ Street, data = house_sales)
boxplot(house_sales$SalesPrice ~ house_sales$Street)
house_sales$SalesPrice
house_sales = read.csv("rData/MA321_Data/house-data.csv")
View(house_sales)
########House Data########
#https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques
house_sales = read.csv("rData/MA321_Data/house-data.csv")
house_sales.r.sp = autoModel("SalePrice", house_sales)
boxplot(house_sales$SalePrice ~ house_sales$Street)
boxplot(dd$protest ~ house_sales$system)
View(house_sales)
house_sales_PR = house_sales[house_sales$Street == "Pave",]
boxplot(house_sales$SalePrice ~ house_sales$Street)
boxplot(house_sales_PR$SalePrice ~ house_sales$Street)
boxplot(house_sales_PR$SalePrice ~ house_sales_PR$Street)
house_sales_NPR = house_sales[house_sales$Street != "Pave",]
t.test(house_sales_PR$SalePrice, house_sales_NPR$SalePrice)
#given our package made isn't on CRAN and we have it locally, we need to install from the zip file using devtools
library(devtools)
install_local("automodel.zip")
library(automodel) #the function made during this dissertation, all code is within 'automodel.R'
########Titanic Run########
#https://www.kaggle.com/c/titanic
titanicData = read.csv("rData/TitanicData/train.csv")
titanicData.r = autoModel("Survived", titanicData)
View(titanicData.r)
#results
titanicData.r$cleanData
#results
temp = titanicData.r$cleanData
View(titanicData)
View(temp)
summary(titanicData.r$linearRegression$model)
summary(titanicData.r$kMeansResults$model)
summary(titanicData.r$kMeansResults$model$cluster)
detach("package:automodel", unload = TRUE)
#given our package made isn't on CRAN and we have it locally, we need to install from the zip file using devtools
library(devtools)
install_local("automodel.zip")
library(automodel) #the function made during this dissertation, all code is within 'automodel.R'
########Titanic Run########
#https://www.kaggle.com/c/titanic
titanicData = read.csv("rData/TitanicData/train.csv")
titanicData.r = autoModel("Survived", titanicData)
#results
temp = titanicData.r$cleanData
View(titanicData.r)
View(temp)
View(titanicData)
summary(titanicData.r$linearRegression$model)
########House Data########
#https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques
house_sales = read.csv("rData/MA321_Data/house-data.csv")
#run the model for two different variables
house_sales.r.oc = autoModel("OverallCond", house_sales)
house_sales.r.sp = autoModel("SalePrice", house_sales)
summary(titanicData.r$linearRegression$model)
