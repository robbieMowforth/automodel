load("exam_data.RData")
load("C:/Users/robbi/FileSync/UniProjects/analysis/exam_data.RData")
View(d1)
summary(d1)
#first lets have a look at the summary stats
summary(d1)
summary(d2)
View(d2)
#first lets have a look at the summary stats
summary(d1)
par(mfrow=c(3,1))
boxplot(d1$mark)
boxplot(d1$coursework)
boxplot(d1$exam)
par(mfrow=c(1,3))
boxplot(d1$mark)
boxplot(d1$coursework)
boxplot(d1$exam)
boxplot(d1$mark, lab = "mark")
?boxplot()
boxplot(d1$mark, xlab = "mark")
par(mfrow=c(1,3))
boxplot(d1$mark, xlab = "mark")
boxplot(d1$coursework, xlab = "coursework")
boxplot(d1$exam, xlab = "exam")
library(rethinking)
d <- read.csv("C:/Users/robbi/FileSync/UniProjects/analysis/caffeine_data.csv")
summary(d)
### Q1.1
d$hi_caf <- ifelse(d$caffeine == 4, 1, 0)
model_spec1 <- alist(GRE_change ~ dnorm(mu, sigma),
mu <- beta0 + beta1 * hi_caf,
beta0 ~ dnorm(0, 10),
beta1 ~ dnorm(0, 5),
sigma ~ dnorm(0, 10))
model1 <- quap(model_spec1, data = d)
library(rethinking)
d <- read.csv("C:/Users/robbi/FileSync/UniProjects/analysis/caffeine_data.csv")
packages.install("rethinking")
package.install("rethinking")
install.packages("rethinking")
library(rethinking)
install.packages("rethinking")
library(tidyverse) #Gives us useful functions like 'select()'
library(stringr) # used for detecting string within another string on vectors
library(haven) #used to read the dta file
library(labelled) #reads a data labels, sjmisc, frq function
library(mice) # used for multiple imputation missing data
library(Hmisc)
library(caret) #used for the ridge, lasso, elastic net
library(glmnet) #used for the ridge, lasso, elastic net
library(broom) #used to sort regression models
library(ggcorrplot) #used to plot corr matrix
library(class) #this is used for knn
library(car) #needed for the multi variable linear regression plots
library(sqldf) #used for SQL queries on the starting tables (to transform)
#The code for the analysis on the data is within the other file which we load in here
source("/Users/robbi/OneDrive/Documents/UniPaper3.0/rCode/transformFunction.R")
#In our data we have missing values re-coded as various negative values, which are:
#-9, missing
#-8, inapplicable
#-7, proxy respondent
#-2, refused
#-1, don't know
#We should re-code this all to 'NA' to allow for the model to compile, here a function to achieve this
recodeNA = function(data){
data[data == -9] = NA
data[data == -8] = NA
data[data == -7] = NA
data[data == -2] = NA
data[data == -1] = NA
return(data)
}
#This function below is used so that we can join the income data into our model in a way were it keeps 'pidp' as a primary key
#here what we are doing is that for each seen finance category, we are creating a binary column stating if the respective
#pidp is associated with a ficode, (1 means yes, 0 means no)
#here we are also building the select query used for the SQL manipulation later on
sqlTransform = function(data){
#start of the SQL query
querySQL = "SELECT pidp,SUM(frmnthimp_dv) as frmnthimp_dv_total"
#for loop which turns all ficodes into binary columns
for (i in 1:max(data$ficode)){
colName = paste("ficode", i,sep="")
data[[colName]] = ifelse(data$ficode == i, 1, 0)
querySQL = paste(querySQL,",SUM(",colName,") as ",colName,sep = "")
}
#finish of the SQL query
querySQL = paste(querySQL,"FROM data GROUP BY pidp")
#SQL to turn the table into version where pidp is unique per row
return(sqldf(querySQL))
}
#This function is used to analyse the distribution of GHQ scores and correlations of questions in a dataset
ghq_analyze = function(data) {
#We can have a look at the variable scghq1_dv (total GHQ score).
mainScore = select(data,contains("ghq1"))
print(summary(mainScore))
#get number of non-NA Observations
print(paste(nrow(mainScore) - sum(is.na(mainScore)),"non NA observations"))
#Get the mode of the total score
mstComm.an = as.numeric(names(which(table(mainScore) == max(table(mainScore)))))
print(paste("Mode/most frequent score is:",mstComm.an))
#getting the total score column and transforming into clean data
totalScores = as.numeric(unlist(mainScore))
#plotting histogram
par(ask = TRUE)
plot(table(totalScores),
xlab = "Result of GHQ questionnaire",
xaxt = 'n',
ylab = "Frequency",
main = paste("GHQ Results Histogram (",deparse(substitute(data)),")",sep=""))
axis(1,at = seq(0,36,6))
?qqplot()
#plotting a Q-Q Plot to investigate normality of distribution
qqPlot(totalScores,
ylab = "GHQ Total Score",
main = paste("GHQ Results Q-Q Plot (",deparse(substitute(data)),")",sep=""))
#getting the ghq question ONLY (no total scores)
x_ghq = select(data,contains("ghq"))
x_ghq = select(x_ghq,-contains("ghq1"))
x_ghq = select(x_ghq,-contains("ghq2"))
#summary of all questions
print(summary(x_ghq))
#NA removal
x_ghq = na.omit(x_ghq)
#creating the correlation matrix and plotting
corr_ghq = cor(x_ghq)
ggcorrplot(corr_ghq,
type = "lower",
lab = T,
title = paste("GHQ Questions Corr Plot (",deparse(substitute(data)),")",sep=""))
}
#this function is used to clean out the ghq question from the data so that we are left with just the total scores
#We also then move all common known columns which are totals of other columns to the end of the dataset so that they are kept in corr checks
ghq_clean_move = function(data) {
#chaging the name of the ghq total score and removing the questions from the dataset
names(data)[names(data) == names(select(data,contains("ghq1")))] = "total_score"
data = select(data,-contains("ghq"))
#re-ordering the columns for the correlation check so that we automatically keep all 'val' variables
for (i in 1:length(names(data))) {
if (grepl("val",colnames(data)[i], fixed = T) | grepl("_dv",colnames(data)[i], fixed = T)) {
data = data %>% relocate(colnames(data)[i], .after = last_col())
}
}
#return the cleaned dataset
return(data)
}
#We use this function to add prediction intervals which are unique to GHQ
predGHQadd = function(modelResults,ghq_side){
modelResults = w2indresp.r
ghq_side = grSide
#get prediction sheets
modelKMeans = modelResults$kMeansResults$predictions
modelkNN = as.data.frame(modelResults$kNNResults$predictions)
modelLinear = modelResults$linearRegression$predictions
modelElastic = modelResults$elasticNetRegression$predictions
#creating 3-range score tables for K-Means using function
addGroupScore = function(modelPred) {
modelPred$groupScore = 0
for (i in 1:nrow(modelPred)) {
if (modelPred[i,2] != 0 & modelPred[i,2] != 36) {
modelPred[i,4] = modelPred[i-1,3] + modelPred[i,3] + modelPred[i+1,3]
}
}
return(modelPred)
}
#adding the new columns
modelKMeans = addGroupScore(modelKMeans)
modelkNN = addGroupScore(modelkNN)
#creating the 1 unit bounds
modelLinear$unitlwr = modelLinear$fit - 1
modelLinear$unitupr = modelLinear$fit + 1
modelElastic$unitlwr = modelElastic$fit - 1
modelElastic$unitupr = modelElastic$fit + 1
#creating the GHQ bounds
modelLinear$GHQlwr = modelLinear$fit - ghq_side
modelLinear$GHQupr = modelLinear$fit + ghq_side
modelElastic$GHQlwr = modelElastic$fit - ghq_side
modelElastic$GHQupr = modelElastic$fit + ghq_side
#getting prediction accuracy in group for K-Means
clusterKMeans = 0
for (i in unique(modelKMeans$cluster)){
temp = subset(modelKMeans,cluster == i)
clusterKMeans = clusterKMeans + max(temp$groupScore)
}
predkNN = 0
for (i in unique(modelkNN$predicted)){
temp = subset(modelkNN,predicted == i)
for (j in 1:nrow(temp)) {
if (temp$predicted[j] == temp$real[j]) {
predkNN = predkNN + temp$groupScore[j]
}
}
}
print(paste("The K-Means Model predicts within the 1 unit interval range with accuracy of",
round(clusterKMeans/sum(modelKMeans$freq),4)))
print(paste("The kNN Model predicts within the 1 unit interval range with accuracy of",
round(predkNN/sum(modelkNN$freq),4)))
#creating the prediction for the (Linear): 1 unit
print(paste("The Linear Model predicts within the 1 unit interval with accuracy of",
round(sum(modelLinear$real >= round(modelLinear$unitlwr,0) & modelLinear$real <= round(modelLinear$unitupr,0))/length(modelLinear$fit),4)))
#creating the prediction for the (Linear): GHQ limits
print(paste("The Linear Model predicts within the GHQ calculated interval with accuracy of",
round(sum(modelLinear$real >= round(modelLinear$GHQlwr,0) & modelLinear$real <= round(modelLinear$GHQupr,0))/length(modelLinear$fit),4)))
#creating the prediction for the (Elastic): 1 unit
print(paste("The Elastic Model predicts within the 1 unit interval with accuracy of",
round(sum(modelElastic$real >= round(modelElastic$unitlwr,0) & modelElastic$real <= round(modelElastic$unitupr,0))/length(modelElastic$fit),4)))
#creating the prediction for the (Elastic): GHQ limits
print(paste("The Elastic Model predicts within the GHQ calculated interval with accuracy of",
round(sum(modelElastic$real >= round(modelElastic$GHQlwr,0) & modelElastic$real <= round(modelElastic$GHQupr,0))/length(modelElastic$fit),4)))
#create a list which holds both the sheets
predictionSheets = list(modelKMeans,modelkNN,modelLinear,modelElastic)
names(predictionSheets) = c("K-Means","kNN","Linear","Elastic")
#returning the sheets separately
return(predictionSheets)
}
#We can set a working directory of pre-selected data
#(would recommend file.choose() in future)
setwd("/Users/robbi/OneDrive/Documents/UniPaper3.0/rData/selected")
#getting the wave 2 data
w2income = read_table("b_income.tab") #Income and payment
w2indresp = read_table("b_indresp.tab") #All information from the individual questionnaires
#getting the wave 3 data
w3income = read_table("c_income.tab") #Income and payment
w3indresp = read_table("c_indresp.tab") #All information from the individual questionnaires
#getting Nurse visit mixed data
#'xindresp_ns.tab' is Wave 2 and 3 Nurse - individual respondents data
mixNurse = as.data.frame(read_table(file = "xindresp_ns.tab"))
#Blood sample from nurse visit, 2010-2011 collected, 2013 analyzed
mixBloodData = as.data.frame(read_table(file = "xlabblood_ns.tab"))
#re-code the NA values
w2income = recodeNA(w2income)
w2indresp = recodeNA(w2indresp)
w3income = recodeNA(w3income)
w3indresp = recodeNA(w3indresp)
mixNurse = recodeNA(mixNurse)
mixBloodData = recodeNA(mixBloodData)
#we have to remove the GHQ related variables from the nurse data to avoid duplicate predictor columns
#making sure we are only getting relevant columns with the function
names(select(mixNurse,contains("ghq")))
#removing the questions and making a new dataset for it
joinSurveyData = select(mixNurse,-contains("ghq"))
#first, we need to remove some wave 2 & 3 naming conventions to assist when merging
names(w2income) = str_replace(names(w2income),"b_","")
names(w2indresp) = str_replace(names(w2indresp),"b_","")
names(w3income) = str_replace(names(w3income),"c_","")
names(w3indresp) = str_replace(names(w3indresp),"c_","")
###INVESTIGATE THE DIFFERENCE###
names(w3indresp)[!names(w3indresp) %in% names(w2indresp)]
#adding a wave column to the survey data for joining
w2indresp$wave = 2
w3indresp$wave = 3
#next we have to join the survey data and income data together per wave 2 and 3, this requires some SQL
#merge income with nurse visit data (Note this use of sqlTransform here)
w2Merge = merge(w2indresp,sqlTransform(w2income), by = "pidp")
#now doing the same for wave 3
w3Merge = merge(w3indresp,sqlTransform(w3income), by = "pidp")
#Lastly, we can now make all the joins between the relevant wave data. We are only going to use INNER JOIN's
#(reason is because analyze as clusters, LEFT JOIN would leave NA's for alot, doesn't share fair comparison)
w2MergeNurse = merge(x = w2Merge, y = joinSurveyData, by = c("pidp","wave")) #above with nurse visit data
w2MergeNurseBlood = merge(w2MergeNurse, y = mixBloodData, by = c("pidp","wave")) #above with blood samples
w3MergeNurse = merge(x = w3Merge, y = joinSurveyData, by = c("pidp","wave")) #above with nurse visit data
w3MergeNurseBlood = merge(w3MergeNurse, y = mixBloodData, by = c("pidp","wave")) #above with blood samples
mixNurseBlood = merge(x = mixNurse, y = mixBloodData, by = c("pidp","wave"))
#to assist this calculation, we need some variables to hold to two GHQ columns
ghq1_dv = na.omit(w2indresp$scghq1_dv)
ghq2_dv = na.omit(w2indresp$scghq2_dv)
#amount of groups in the ghq score
grGHQ = length(unique(ghq2_dv))
grGHQ
#amount of total scores possible in ghq score
scGHQ = length(unique(ghq1_dv))
scGHQ
#estimate size of each group in ghq2
grSize = scGHQ / grGHQ
grSize
#how far we should look each side of the predicted values
grSide = grSize / 2
grSide
#to compare our prediction accuracy on this data, I want to compare to:
#randomly picking a GHQ score if all scores assumed equal
randomPickCh.g = 1/length(unique(ghq1_dv))
randomPickCh.g
#then the chance of always picking the most common score in scghq
mstComm.g = as.numeric(names(which(table(ghq1_dv) == max(table(ghq1_dv)))))
paste("Mode/most frequent score is:",mstComm.g)
modePickCh.g = max(table(ghq1_dv)) / length(ghq1_dv)
modePickCh.g
#The chance when randomly picking within a +- 1 range (so randomly picking 7 is correct if real is in (6,7,8))
randomRangeCh.g = ((3*37)-2)/(length(unique(ghq1_dv))*37)
randomRangeCh.g
#The chance of picking within the largest +- 1 range (which is 6,7,8)
rangePickCh.g = sum(table(ghq1_dv)[c(mstComm.g + 1, mstComm.g + 2, mstComm.g + 3)]) / length(ghq1_dv)
rangePickCh.g
#For write up, we want to set the wd to the console ouput locatiob
setwd("/Users/robbi/OneDrive/Documents/uniPaper3.0/consoleOutput")
########Titanic Run########
source("/Users/robbi/OneDrive/Documents/UniPaper3.0/rCode/transformFunction.R")
setwd("/Users/robbi/OneDrive/Documents/uniPaper3.0/rData/TitanicData")
#Load in the Titanic data (notice we load in 'train.csv', that because the test data Kaggle provides doesn't have 'survived' values)
titanicData = read.csv("train.csv")
#example that x variables can be factors (meaning we can have sting in titles kept)
titanicData$Sex = as.factor(titanicData$Sex)
#get the random chances of picking Survival we are wanting to beat with our models
#complete random
randomCh.t = 1/length(unique(titanicData$Survived))
randomCh.t
#mode selection
mstComm.t = as.numeric(names(which(table(titanicData$Survived) == max(table(titanicData$Survived)))))
paste("Mode/most frequent score is:",mstComm.t)
modePickCh.t = max(table(titanicData$Survived)) / length(titanicData$Survived)
modePickCh.t
